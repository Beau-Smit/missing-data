{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulate_data\n",
    "import remove_data\n",
    "import matrix_completion\n",
    "import trunc_nnm\n",
    "import errors\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.154555</td>\n",
       "      <td>0.064260</td>\n",
       "      <td>0.070888</td>\n",
       "      <td>0.119523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025790</td>\n",
       "      <td>0.073764</td>\n",
       "      <td>0.063360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.076481</td>\n",
       "      <td>0.115322</td>\n",
       "      <td>0.070888</td>\n",
       "      <td>0.119523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.101535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108082</td>\n",
       "      <td>0.058559</td>\n",
       "      <td>0.114662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046207</td>\n",
       "      <td>0.039233</td>\n",
       "      <td>0.070888</td>\n",
       "      <td>0.119523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.101535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009263</td>\n",
       "      <td>0.037587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070108</td>\n",
       "      <td>0.125841</td>\n",
       "      <td>0.070888</td>\n",
       "      <td>0.119523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.101535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.076243</td>\n",
       "      <td>0.141776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101535</td>\n",
       "      <td>0.404643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066675</td>\n",
       "      <td>0.079655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3    4         5         6         7   \\\n",
       "0  0.154555  0.064260  0.070888  0.119523  0.0  0.000000  0.101535  0.000000   \n",
       "1  0.076481  0.115322  0.070888  0.119523  0.0  0.136083  0.101535  0.000000   \n",
       "2  0.046207  0.039233  0.070888  0.119523  0.0  0.136083  0.101535  0.000000   \n",
       "3  0.070108  0.125841  0.070888  0.119523  0.0  0.136083  0.101535  0.000000   \n",
       "4  0.043021  0.076243  0.141776  0.000000  0.0  0.000000  0.101535  0.404643   \n",
       "\n",
       "         8         9         10  \n",
       "0 -0.025790  0.073764  0.063360  \n",
       "1  0.108082  0.058559  0.114662  \n",
       "2  0.000000  0.009263  0.037587  \n",
       "3  0.161677  0.000000  0.123666  \n",
       "4  0.000000  0.066675  0.079655  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulate data\n",
    "sim_df = simulate_data.sim_data(**simulate_data.sim_params)\n",
    "\n",
    "# drop categorical feature for now\n",
    "sim_df = sim_df.drop(\"Group\", axis=1)\n",
    "\n",
    "# Normalize data by feature (axis=0)\n",
    "# norms we could use for re-scaling later\n",
    "sim_norm, norms = normalize(sim_df, axis=0, return_norm=True)\n",
    "pd.DataFrame(sim_norm).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNM parameters\n",
    "parameters = {\"eps_outer\": 1e-6,\n",
    "              \"eps_inner\": 1e-6,\n",
    "              \"beta\": 1,\n",
    "              \"max_iter_outer\": 1000,\n",
    "              \"max_iter_inner\": 1000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do error rates vary by rate of missingness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f28822874950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0msim_recovered_nnm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrunc_nnm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_NNM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mnnm_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_norm\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msim_recovered_nnm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\UChicago\\Fall_2021\\35300_Math_for_ML\\final_project\\missing-data\\trunc_nnm.py\u001b[0m in \u001b[0;36mtruncated_NNM\u001b[1;34m(rank, params, Xobs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Perform ADMM minimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mnew_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mADMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOmega\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0morig_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fro'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps_outer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\UChicago\\Fall_2021\\35300_Math_for_ML\\final_project\\missing-data\\trunc_nnm.py\u001b[0m in \u001b[0;36mADMM\u001b[1;34m(A, B, X, params, Xobs, Omega)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_iter_inner\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# Update X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mX_k_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshrinkage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_k\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mY_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# update W\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\UChicago\\Fall_2021\\35300_Math_for_ML\\final_project\\missing-data\\trunc_nnm.py\u001b[0m in \u001b[0;36mshrinkage\u001b[1;34m(X, tau)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mshrinkage\u001b[0m \u001b[0mD\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtruncated\u001b[0m \u001b[0mNNM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     '''\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV_T\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m# shrink S\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mS_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtau\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m         \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->DdD'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->ddd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1660\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1661\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_lst = []\n",
    "\n",
    "for missingness_pct in np.arange(0, 100, 5):\n",
    "    sim_obs = remove_data.missing_at_random(sim_norm, perc_remove=missingness_pct, rand_seed=23)\n",
    "    \n",
    "    sim_recovered_svt = matrix_completion.svt(sim_obs, tau=5)\n",
    "    svt_error = la.norm(sim_norm - sim_recovered_svt, ord='fro')\n",
    "\n",
    "#     rank = la.matrix_rank(sim_norm)\n",
    "#     sim_recovered_nnm = trunc_nnm.truncated_NNM(rank, parameters, sim_obs)\n",
    "#     nnm_error = la.norm(sim_norm - sim_recovered_nnm, ord='fro')\n",
    "    \n",
    "    nnm_error=0\n",
    "    results_lst.append([missingness_pct, svt_error, nnm_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_lst, columns=[\"missingness\", \"SVT\", \"NNM\"]).set_index(\"missingness\")\n",
    "\n",
    "errors.line_plot(results_df.index, results_df.SVT, results_df.NNM, \n",
    "                 save=True,\n",
    "                 title=\"Error Rate vs Missingness\",\n",
    "                 xlabel=\"Percent of data missing\",\n",
    "                 ylabel=\"Reconstruction error (Frobenius norm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rates increased sharply from no missingness to about 10% missingness, then increased at a lesser rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does missingness affect error rates by rank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingness_rank_df = pd.DataFrame(np.arange(0, 100, 5).tolist(), columns=[\"missingness\"])\n",
    "\n",
    "for rank in range(1, 11):\n",
    "    error_lst = []\n",
    "    for missingness in np.arange(0, 100, 5):\n",
    "        # create rank r matrix\n",
    "        sim_arr = simulate_data.create_rank_r_matrix(rank, 100, 10)\n",
    "\n",
    "        # normalize\n",
    "#         sim_norm, norms = normalize(sim_arr, axis=0, return_norm=True)\n",
    "\n",
    "        # remove data at random\n",
    "        sim_obs = remove_data.missing_at_random(sim_arr, perc_remove=missingness, rand_seed=23)\n",
    "\n",
    "        sim_recovered_svt = matrix_completion.svt(sim_obs, tau=5)\n",
    "        svt_error = la.norm(sim_arr - sim_recovered_svt, ord='fro')\n",
    "        \n",
    "        error_lst.append(svt_error)\n",
    "\n",
    "    missingness_rank_df[f\"rank_{rank}\"] = error_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(results_lst, columns=[\"missingness\", \"SVT\", \"NNM\"]).set_index(\"missingness\")\n",
    "missingness_rank_df.set_index(\"missingness\").plot(title=\"Reconstruction Error by Rank and Missingness\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we hold the error rate at 40% and change $\\tau$ for SVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_lst = []\n",
    "\n",
    "for tau in np.arange(0, 10, 1):\n",
    "    sim_obs = remove_data.missing_at_random(sim_norm, perc_remove=40, rand_seed=23)\n",
    "    \n",
    "    sim_recovered_svt = matrix_completion.svt(sim_obs, tau=tau)\n",
    "    svt_error = la.norm(np.round(sim_norm, 4) - np.round(sim_recovered_svt, 4), ord='fro')\n",
    "    \n",
    "    tau_lst.append([tau, svt_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_df = pd.DataFrame(tau_lst, columns=[\"tau\", \"error\"]).set_index(\"tau\")\n",
    "tau_df.plot(title=\"Error rate depending on tau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVT seems to want $\\tau$ to be somewhere midway between 0 and the rank of the original matrix. Should test on other data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do error rates vary by rank of the matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNM parameters\n",
    "parameters = {\"eps_outer\": 1e-6,\n",
    "              \"eps_inner\": 1e-6,\n",
    "              \"beta\": 1,\n",
    "              \"max_iter_outer\": 1000,\n",
    "              \"max_iter_inner\": 1000}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we vary rank for a synthetic dataset with 10% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lst = []\n",
    "n = 200\n",
    "p = 10\n",
    "\n",
    "for rank in [2]:\n",
    "\n",
    "    # Produce r-rank data\n",
    "    sim_norm = simulate_data.create_rank_r_matrix(rank, n, p)\n",
    "    # Set to only 10% missing\n",
    "    sim_obs = remove_data.missing_at_random(sim_norm, perc_remove=10, rand_seed=23)\n",
    "    \n",
    "    print(sim_obs)\n",
    "    sim_recovered_svt = matrix_completion.svt(sim_obs, tau=5)\n",
    "    svt_error = la.norm(sim_norm - sim_recovered_svt, ord='fro')\n",
    "#     print(sim_recovered_svt.astype(int))\n",
    "    \n",
    "    print(sim_obs)\n",
    "    sim_recovered_nnm = trunc_nnm.truncated_NNM(rank, parameters, sim_obs)\n",
    "    nnm_error = la.norm(sim_norm - sim_recovered_nnm, ord='fro')\n",
    "    \n",
    "#     sim_recovered_nnm = trunc_nnm.truncated_NNM(rank, parameters, sim_obs)\n",
    "#     nnm_error = la.norm(sim_norm - sim_recovered_nnm, ord='fro')\n",
    "    \n",
    "    results_lst.append([rank, svt_error, nnm_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_lst, columns=[\"rank\", \"SVT\", \"NNM\"]).set_index(\"rank\")\n",
    "\n",
    "errors.line_plot(results_df.index, results_df.SVT, results_df.NNM, \n",
    "                 save=True,\n",
    "                 title=\"Error Rate vs Matrix Rank\",\n",
    "                 xlabel=\"Rank of the original matrix\",\n",
    "                 ylabel=\"Reconstruction error (Frobenius norm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction error increases fairly linearly with rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do error rates vary by the type of missingness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the last column\n",
    "original_normalized_arr = sim_norm[:,:10]\n",
    "original_normalized_df = pd.DataFrame(original_normalized_arr, columns=sim_df.columns[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionally_remove_data(df, perc_missing):\n",
    "\n",
    "    # make Income missing conditional on age\n",
    "    sim_df = remove_data.missing_conditional_continuous(df, \n",
    "                                                     \"Income\", \n",
    "                                                     \"Income\", \n",
    "                                                     percent_missing=perc_missing)\n",
    "\n",
    "    # make Age missing conditional on itself\n",
    "    sim_df = remove_data.missing_conditional_continuous(sim_df, \n",
    "                                                     \"Age\", \n",
    "                                                     \"Age\", \n",
    "                                                     percent_missing=perc_missing)\n",
    "\n",
    "    # make NChild missing conditional on Continuous_EvenLikelihood_0\n",
    "    sim_df = remove_data.missing_conditional_continuous(sim_df, \n",
    "                                                     \"NChild\", \n",
    "                                                     \"Continuous_EvenLikelihood_0\", \n",
    "                                                     percent_missing=perc_missing)\n",
    "\n",
    "\n",
    "    # # make Continuous_LowLikelihood_0 missing conditional on Continuous_HighLikelihood_0\n",
    "    sim_df = remove_data.missing_conditional_continuous(sim_df, \n",
    "                                                     \"Continuous_LowLikelihood_0\", \n",
    "                                                     \"Continuous_HighLikelihood_0\", \n",
    "                                                     percent_missing=perc_missing)\n",
    "\n",
    "    # # make Continuous_EvenLikelihood_0 missing conditional on itself\n",
    "    sim_df = remove_data.missing_conditional_continuous(sim_df, \n",
    "                                                     \"Continuous_EvenLikelihood_0\", \n",
    "                                                     \"Continuous_EvenLikelihood_0\", \n",
    "                                                     percent_missing=perc_missing)\n",
    "\n",
    "    # # make Continuous_HighLikelihood_0 missing conditional on itself\n",
    "    sim_df = remove_data.missing_conditional_continuous(sim_df, \n",
    "                                                     \"Continuous_HighLikelihood_0\", \n",
    "                                                     \"Continuous_HighLikelihood_0\", \n",
    "                                                     percent_missing=perc_missing)\n",
    "\n",
    "    # conditionally remove entries from each discrete variable\n",
    "    sim_df = remove_data.missing_conditional_discrete(sim_df, \n",
    "                                                    'InCensus', perc_missing)\n",
    "    sim_df = remove_data.missing_conditional_discrete(sim_df, \n",
    "                                                    'Discrete_LowLikelihood_0', perc_missing)\n",
    "    sim_df = remove_data.missing_conditional_discrete(sim_df, \n",
    "                                                    'Discrete_EvenLikelihood_0', perc_missing)\n",
    "    sim_df = remove_data.missing_conditional_discrete(sim_df, \n",
    "                                                    'Discrete_HighLikelihood_0', perc_missing)\n",
    "    sim_df = remove_data.missing_conditional_discrete(sim_df, \n",
    "                                                    'Discrete_LowLikelihood_0', perc_missing)\n",
    "    \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the conditionally_remove_data function\n",
    "sim_conditional_missing = conditionally_remove_data(original_normalized_df, perc_missing=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that Continuous_LowLikelihood_0 is missing for those with low values of Continuous_HighLikelihood_0\n",
    "sim_conditional_missing[['Continuous_HighLikelihood_0', 'Continuous_LowLikelihood_0']].sort_values(\"Continuous_HighLikelihood_0\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total missingness in conditional missing\n",
    "conditional_total_missingness = pd.DataFrame(sim_conditional_missing).isnull().sum().sum() / pd.DataFrame(sim_conditional_missing).size\n",
    "print(conditional_total_missingness)\n",
    "\n",
    "# view missingness for features in conditional missing\n",
    "sim_conditional_missing.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create randomly missing data matching the missingness of the conditional data\n",
    "sim_random_missing = remove_data.missing_at_random(original_normalized_arr, perc_remove=conditional_total_missingness*100, rand_seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total missingness in random missing\n",
    "print(pd.DataFrame(sim_random_missing).isnull().sum().sum() / pd.DataFrame(sim_random_missing).size)\n",
    "\n",
    "# view missingness for features in random missing\n",
    "pd.DataFrame(sim_random_missing, columns=original_normalized_df.columns).isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lst = []\n",
    "\n",
    "for missingness_pct in range(0, 100, 5):\n",
    "    sim_conditional_missing = conditionally_remove_data(original_normalized_df, perc_missing=missingness_pct)\n",
    "    \n",
    "    # check total missingness in conditional missing\n",
    "    conditional_total_missingness = pd.DataFrame(sim_conditional_missing).isnull().sum().sum() / pd.DataFrame(sim_conditional_missing).size\n",
    "    \n",
    "    # create randomly missing data matching the missingness of the conditional data\n",
    "    sim_random_missing = remove_data.missing_at_random(original_normalized_arr, perc_remove=conditional_total_missingness*100, rand_seed=23)\n",
    "\n",
    "    # random missing\n",
    "    sim_recovered_random_svt = matrix_completion.svt(sim_random_missing, tau=5)\n",
    "    random_svt_error = la.norm(np.round(original_normalized_arr, 4) - np.round(sim_recovered_random_svt, 4), ord='fro')\n",
    "\n",
    "#     sim_recovered_random_nnm = trunc_nnm.truncated_NNM(10, parameters, sim_random_missing)\n",
    "#     random_nnm_error = la.norm(np.round(original_normalized_data, 4) - np.round(sim_recovered_random_nnm, 4), ord='fro')\n",
    "\n",
    "    # conditional missing\n",
    "    sim_recovered_conditional_svt = matrix_completion.svt(sim_conditional_missing, tau=5)\n",
    "    conditional_svt_error = la.norm(np.round(original_normalized_arr, 4) - np.round(sim_recovered_conditional_svt, 4), ord='fro')\n",
    "\n",
    "#     sim_recovered_conditional_nnm = trunc_nnm.truncated_NNM(10, parameters, sim_conditional_missing)\n",
    "#     conditional_nnm_error = la.norm(np.round(original_normalized_data, 4) - np.round(sim_recovered_conditional_nnm, 4), ord='fro')\n",
    "\n",
    "    results_lst.append([conditional_total_missingness, random_svt_error, conditional_svt_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svt_errors = pd.DataFrame(results_lst, columns=['missingness','random','conditional']).set_index('missingness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svt_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.line_plot(svt_errors.index*100, svt_errors.random, svt_errors.conditional,\n",
    "                 save=True,\n",
    "                 title=\"Error Rate vs Type of Missingness\",\n",
    "                 xlabel=\"Percent of data missing\",\n",
    "                 ylabel=\"Reconstruction error (Frobenius norm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate for the randomly missing data is strictly greater than the error rate for conditionally missing data. This is consistent across many randomizations. Though this finding may be particular to the data and implementation of removing values, we have a theory that helps explain this phenomenon. When entries are missing based on features within the data, the missingness has structure. In other words, being missing may indicate something about the entry (high or low value) or about another feature that is related to it. Introducing this structure into the data may help the SVT algorithm slightly by indicating subtle information about the missing entries. In contrast, entries that are missing at random do not signal anything about the missing value.\n",
    "\n",
    "With both randomly missing data and conditionally missing data, the error rates increase quickly going from no missing data to about 10% missing. Then, the error rate increases relatively linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How were the missing data produced?\n",
    "\n",
    "Both the randomly missing data and the conditionally missing data were constructed from the same synthetic dataset. The randomly missing dataset was constructed by selecting random elements to drop from the data. Therefore, in expectation, each feature should have the same amount of missingness. \n",
    "\n",
    "The conditional data was generated by one of two processes depending on whether the feature is continuous or binary. For continuous features, values were removed probablistically based on values within the feature or values from another feature. For instance, Income values could be more likely missing if income is a low value. Alternatively, Income values could be removed based on whether age is a high or low value. For binary features, we randomly assigned different likelihoods of missingness to values of 0 and 1. Thus, values of 1 may be removed at a higher rate than values of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6ab49f5d33c3ce7301436240baf1d68fecd837c84762ac5739e8937c0d65714"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
